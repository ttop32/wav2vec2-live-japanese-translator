{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72bcbeac-10c2-4e2f-af45-a93321976b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Reusing dataset common_voice (/home/user/.cache/huggingface/datasets/common_voice/ja/6.1.0/078d412587e9efeb0ae2e574da99c31e18844c496008d53dc5c60f4159ed639b)\n",
      "/home/user/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "/home/user/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "/home/user/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated method getConverter. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "Parameter 'function'=<function preprocessData at 0x7fc8fdadee60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1cb93920104b82ad589d4be4654a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/632 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:40: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4614338428428c92df19a1ca3f1700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/tf2/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1631630836880/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 22.082063\n",
      "CER: 10.081756\n"
     ]
    }
   ],
   "source": [
    "#test common_voice test data\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import re\n",
    "import pykakasi \n",
    "import MeCab\n",
    "\n",
    "\n",
    "wer = load_metric(\"wer\")\n",
    "cer = load_metric(\"cer\")\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"wav2vec2_large_xlsr_japanese_hiragana\").to(\"cuda\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"wav2vec2_large_xlsr_japanese_hiragana\")\n",
    "test_dataset = load_dataset(\"common_voice\", \"ja\", split=\"test\")\n",
    "\n",
    "\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\‘\\”\\�‘、。．！，・―─~｢｣『』\\\\\\\\※\\[\\]\\{\\}「」〇？…]'\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "kakasi = pykakasi.kakasi()\n",
    "kakasi.setMode(\"J\",\"H\")      # kanji to hiragana\n",
    "kakasi.setMode(\"K\",\"H\")      # katakana to hiragana\n",
    "conv = kakasi.getConverter()\n",
    "\n",
    "\n",
    "FULLWIDTH_TO_HALFWIDTH = str.maketrans(\n",
    "    '　０１２３４５６７８９ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ！゛＃＄％＆（）＊＋、ー。／：；〈＝〉？＠［］＾＿‘｛｜｝～',\n",
    "    ' 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&()*+,-./:;<=>?@[]^_`{|}~',\n",
    ")\n",
    "def fullwidth_to_halfwidth(s):\n",
    "    return s.translate(FULLWIDTH_TO_HALFWIDTH)\n",
    "\n",
    "\n",
    "def preprocessData(batch):\n",
    "    batch[\"sentence\"] = fullwidth_to_halfwidth(batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub(chars_to_ignore_regex,' ', batch[\"sentence\"]).lower()  #remove special char\n",
    "    batch[\"sentence\"] = wakati.parse(batch[\"sentence\"])                              #add space\n",
    "    batch[\"sentence\"] = conv.do(batch[\"sentence\"])                                   #covert to hiragana\n",
    "    batch[\"sentence\"] = \" \".join(batch[\"sentence\"].split())+\" \"                         #remove multiple space \n",
    "    \n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    batch[\"speech\"] = torchaudio.functional.resample(speech_array, sampling_rate, 16000)[0].numpy()    \n",
    "    return batch\n",
    "\n",
    "\n",
    "test_dataset = test_dataset.map(preprocessData)\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to read the aduio files as arrays\n",
    "def evaluate(batch):\n",
    "\tinputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tlogits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n",
    "\n",
    "\tpred_ids = torch.argmax(logits, dim=-1)\n",
    "\tbatch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n",
    "\treturn batch\n",
    "\n",
    "result = test_dataset.map(evaluate, batched=True, batch_size=8)\n",
    "\n",
    "print(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n",
    "print(\"CER: {:2f}\".format(100 * cer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8ef729-b179-4451-8303-8b1b48036dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Reusing dataset common_voice (/home/user/.cache/huggingface/datasets/common_voice/ja/6.1.0/078d412587e9efeb0ae2e574da99c31e18844c496008d53dc5c60f4159ed639b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf7829e0c12494cb17e0f06d3348739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/632 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ['そぼ を おもにきねん を くさい くろころがし て い', 'さいふ を な くし かん の で こうばん で いき ます']\n",
      "Reference: ['祖母は、おおむね機嫌よく、サイコロをころがしている。', '財布をなくしたので、交番へ行きます。']\n"
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "\n",
    "#usage\n",
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"wav2vec2_large_xlsr_japanese_hiragana\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"wav2vec2_large_xlsr_japanese_hiragana\")\n",
    "test_dataset = load_dataset(\"common_voice\", \"ja\", split=\"test\")\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to read the aduio files as arrays\n",
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    batch[\"speech\"] = torchaudio.functional.resample(speech_array, sampling_rate, 16000)[0].numpy()    \n",
    "    return batch\n",
    "\n",
    "\n",
    "test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
    "inputs = processor(test_dataset[:2][\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tlogits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "print(\"Prediction:\", processor.batch_decode(predicted_ids))\n",
    "print(\"Reference:\", test_dataset[:2][\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8bf75-812b-49ef-b18c-e9666ec3213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Cloning https://huggingface.co/ttop324/wav2vec2-live-japanese-translator into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2a084935b34fb48e9fdb2635e88853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"wav2vec2_large_xlsr_japa